{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import h5py\n",
    "import logging\n",
    "from os import path\n",
    "from pandas import HDFStore\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load matplotlib inline extension\n",
    "%matplotlib inline\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = pathlib.Path(\"./fmri\")/\"tensorboard_logs_conv1D\"\n",
    "shutil.rmtree(logdir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-936680f3dc559993\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-936680f3dc559993\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"./fmri/tensorboard_logs_conv1D\" --host 0.0.0.0 --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('fmri/tensorboard_logs_conv1D')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((101, 158, 28), (101,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data from hdf5 file\n",
    "#seed=2020\n",
    "\n",
    "# origin h5 file location, load and shuffle timeseries\n",
    "h5_file = '/data/elekin/data/results/00-EXTRACCION-CEREBELO/elekin_fmri_ts_pandas.hdf5'\n",
    "tr_key = 'cbl/dataset'#transformed dataset (id,time series matrix 158x28, label)\n",
    "dataset = pd.read_hdf(h5_file, tr_key)\n",
    "\n",
    "num_samples = dataset.shape[0]\n",
    "features = np.stack(dataset['features'].values)\n",
    "labels = dataset['label'].values.astype('int8')\n",
    "\n",
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "np.random.seed(seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,labels,test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_earlystop_callback = EarlyStopping(\n",
    "  monitor='val_accuracy', min_delta=0.01,\n",
    "  patience=50)\n",
    "\n",
    "def get_callbacks(name):\n",
    "    return [\n",
    "        tfdocs.modeling.EpochDots(),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n",
    "        tf.keras.callbacks.TensorBoard(logdir/name),\n",
    "        val_earlystop_callback\n",
    "      ]\n",
    "\n",
    "def compile_and_fit(model, name, optimizer=None, max_epochs=1000):\n",
    "    if optimizer is None:\n",
    "        optimizer = get_optimizer()\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', 'binary_crossentropy'])\n",
    "    model.summary()\n",
    "    history = model.fit(X_train, y_train, batch_size=4, use_multiprocessing=True, \n",
    "                            validation_data=(X_test,y_test), epochs=max_epochs, callbacks=get_callbacks(name),\n",
    "                            verbose=0)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many models train better if you gradually reduce the learning rate during training. \n",
    "Use optimizers.schedules to reduce the learning rate over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = 1\n",
    "FEATURES = 158\n",
    "CHANNELS = 28\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.1,\n",
    "  decay_steps=STEPS_PER_EPOCH*1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "\n",
    "def get_optimizer():\n",
    "  return tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed) # set up tensorflow seed\n",
    "tf.keras.backend.clear_session()# avoid impact from previous trainings\n",
    "cnn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=16, kernel_size=3,\n",
    "                       activation=tf.nn.relu,\n",
    "                      input_shape=[FEATURES, CHANNELS]),\n",
    "    tf.keras.layers.MaxPool1D(),\n",
    "    tf.keras.layers.Conv1D(filters=16, kernel_size=3,\n",
    "                           activation=tf.nn.relu),\n",
    "    tf.keras.layers.LSTM(8, activation=tf.nn.tanh),\n",
    "    #tf.keras.layers.SimpleRNN(128, activation=tf.nn.tanh),\n",
    "    #tf.keras.layers.GRU(24, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_histories = {}\n",
    "rnn='lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 156, 16)           1360      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 78, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 76, 16)            784       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,233\n",
      "Trainable params: 3,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0044s vs `on_train_batch_end` time: 0.0137s). Check your callbacks.\n",
      "\n",
      "Epoch: 0, accuracy:0.4429,  binary_crossentropy:0.7300,  loss:0.7300,  val_accuracy:0.4194,  val_binary_crossentropy:0.6999,  val_loss:0.6999,  \n",
      "..........................................................."
     ]
    }
   ],
   "source": [
    "size_histories['cnn/conv1d/'+rnn] = compile_and_fit(cnn, \"cnn/conv1d/\"+rnn, optimizer=None, max_epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
