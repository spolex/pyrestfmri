{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import h5py\n",
    "import logging\n",
    "from os import path\n",
    "from pandas import HDFStore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Load matplotlib inline extension\n",
    "%matplotlib inline\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_base_dir = pathlib.Path(\"./fmri\")/\"tensorboard_logs\"\n",
    "shutil.rmtree(logs_base_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b1a3ed2272b00bdc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b1a3ed2272b00bdc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"./fmri/tensorboard_logs\" --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((101, 158, 28), (101,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data from hdf5 file\n",
    "\n",
    "# origin h5 file location, load and shuffle timeseries\n",
    "h5_file = '/data/elekin/data/results/00-EXTRACCION-CEREBELO/elekin_fmri_ts_pandas.hdf5'\n",
    "\n",
    "#windows\n",
    "#h5_file = \"Z:/elekin/02-RESULTADOS/00-EXTRACCION-CEREBELO/elekin_fmri_ts_pandas.hdf5\"\n",
    "\n",
    "tr_key = 'cbl/dataset'#transformed dataset (id,time series matrix 158x28, label)\n",
    "dataset = pd.read_hdf(h5_file, tr_key)\n",
    "\n",
    "num_samples = dataset.shape[0]\n",
    "features = np.stack(dataset['features'].values)\n",
    "labels = dataset['label'].values.astype('int8')\n",
    "\n",
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((158, 28), ()), types: (tf.float32, tf.int8)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build tensorflow dataset TODO adjust float precission to GPU requirements to see the effect\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = int(len(list(iter(dataset.cache()))))\n",
    "STEPS_PER_EPOCH = 1 #gradient descent regularization\n",
    "FEATURES = features.shape[1] #number of volumes\n",
    "CHANNELS = features.shape[2] #number of time series\n",
    "seed=38 #to keep same random states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 33)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split dataset to training and test\n",
    "train_size = int(0.67 * DATASET_SIZE)\n",
    "test_size = int(0.33 * DATASET_SIZE)\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size = 8\n",
    "np.random.seed(seed) #seting random state\n",
    "shuffle_buffer = 51 #buffer for splitting\n",
    "full_dataset = dataset.shuffle(shuffle_buffer)\n",
    "train_dataset = full_dataset.take(train_size).batch(minibatch_size).prefetch(1).cache() #caching prefetched minibatch\n",
    "test_dataset = full_dataset.skip(train_size).batch(minibatch_size).prefetch(1).cache() #caching prefetched minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i,y in enumerate(test_dataset):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(0.001,decay_steps=STEPS_PER_EPOCH*1000,\n",
    "                                                             decay_rate=1,staircase=False)\n",
    "\n",
    "def get_optimizer():\n",
    "    return tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stop configuration\n",
    "earlystop_callback = EarlyStopping( monitor='val_accuracy', min_delta=0.0001,patience=200)\n",
    "training_earlystop_callback = EarlyStopping( monitor='accuracy', min_delta=0.0001, patience=200)\n",
    "\n",
    "def get_callbacks(name):\n",
    "    return [tfdocs.modeling.EpochDots(), earlystop_callback, tf.keras.callbacks.TensorBoard(logs_base_dir/name)]\n",
    "\n",
    "def compile_and_fit(model, name, optimizer=None, max_epochs=1000):\n",
    "    if optimizer is None:\n",
    "        optimizer = get_optimizer()\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    history = model.fit(train_dataset, use_multiprocessing=True, validation_data=test_dataset, epochs=max_epochs, callbacks=\n",
    "                        get_callbacks(name), verbose=0)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.keras.models.Sequential([\n",
    " tf.keras.layers.LSTM(8, activation=tf.nn.tanh,input_shape=[FEATURES, CHANNELS]),\n",
    " tf.keras.layers.Dense(16, activation=tf.nn.relu),\n",
    " tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    " tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb='lstm/108'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 8)                 1184      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,473\n",
      "Trainable params: 1,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0051s vs `on_train_batch_end` time: 0.0593s). Check your callbacks.\n",
      "\n",
      "Epoch: 0, accuracy:0.5522,  loss:0.6937,  val_accuracy:0.5588,  val_loss:0.6908,  \n",
      "....................................................................................................\n",
      "Epoch: 100, accuracy:0.9552,  loss:0.1934,  val_accuracy:0.8235,  val_loss:0.3986,  \n",
      "....................................................................................................\n",
      "Epoch: 200, accuracy:1.0000,  loss:0.0168,  val_accuracy:0.8235,  val_loss:0.5693,  \n",
      "....................................................................................................\n",
      "Epoch: 300, accuracy:1.0000,  loss:0.0043,  val_accuracy:0.8235,  val_loss:0.6746,  \n",
      "..................."
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(seed)\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "size_histories['lstm/tiny/'+numb] = compile_and_fit(lstm, \"fmri/lstm/tiny/\"+numb, optimizer=None, max_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.7941176295280457\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('test acc:', max(size_histories['lstm/tiny/'+numb].history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PENDING TO REVIEW ------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many models train better if you gradually reduce the learning rate during training. \n",
    "Use optimizers.schedules to reduce the learning rate over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = 1\n",
    "FEATURES = 158\n",
    "CHANNELS = 28\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  1e-6,\n",
    "  decay_steps=STEPS_PER_EPOCH*1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "\n",
    "def get_optimizer():\n",
    "    return tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "lstm = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(24, activation=tf.nn.tanh,input_shape=[FEATURES, CHANNELS]),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])\n",
    "size_histories['lstm/tiny'] = compile_and_fit(lstm, \"lstm/tiny\", optimizer=None, max_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed) # establecemos la semilla para tensorflow\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "small = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True, activation=tf.nn.tanh,input_shape=[FEATURES, CHANNELS]),\n",
    "    tf.keras.layers.LSTM(256, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])\n",
    "size_histories['lstm/small'] = compile_and_fit(small, \"lstm/small\", optimizer=None, max_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss = tfdocs.plots.HistoryPlotter(metric = 'loss', smoothing_std=10)\n",
    "plotter_loss.plot(size_histories)\n",
    "plt.ylim([0., 2.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_acc = tfdocs.plots.HistoryPlotter(metric = 'accuracy', smoothing_std=10)\n",
    "plotter_acc.plot(size_histories)\n",
    "plt.ylim([0.3, 1.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(logs_base_dir/'lstm/regularizers/tiny', ignore_errors=True)\n",
    "shutil.copytree(logs_base_dir/'lstm/tiny', logs_base_dir/'lstm/regularizers/tiny')\n",
    "regularizer_histories = {}\n",
    "#regularizer_histories['tiny'] = size_histories['lstm/tiny']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed) # establecemos la semilla para tensorflow\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "lstm = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True, activation=tf.nn.relu),input_shape=[FEATURES, CHANNELS]),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, activation=tf.nn.relu)),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])\n",
    "regularizer_histories['bi-tiny'] = compile_and_fit(lstm, \"lstm/regularizers/bi-tiny\", optimizer=None, max_epochs=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop out\n",
    "dr=0.2\n",
    "tf.random.set_seed(seed) # establecemos la semilla para tensorflow\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "lstm = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True, activation=tf.nn.tanh,input_shape=[FEATURES, CHANNELS]),\n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.LSTM(128, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)],)\n",
    "regularizer_histories['drop-tiny'] = compile_and_fit(lstm, \"lstm/regularizers/drp-tiny\", optimizer=None, max_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop out\n",
    "dr=0.25\n",
    "lr=1e-6\n",
    "tf.random.set_seed(seed) # establecemos la semilla para tensorflow\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "lstm = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(24, activation=tf.nn.tanh, kernel_regularizer=regularizers.l2(lr), input_shape=[FEATURES, CHANNELS]),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(lr)),\n",
    "    tf.keras.layers.Dense(8, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(lr)),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, kernel_regularizer=regularizers.l2(lr))],)\n",
    "regularizer_histories['kernel'] = compile_and_fit(lstm, \"lstm/regularizers/kernel-reg\", optimizer=None, max_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss = tfdocs.plots.HistoryPlotter(metric = 'loss', smoothing_std=10)\n",
    "plotter_loss.plot(regularizer_histories)\n",
    "plt.ylim([0., 2.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_acc = tfdocs.plots.HistoryPlotter(metric = 'accuracy', smoothing_std=10)\n",
    "plotter_acc.plot(regularizer_histories)\n",
    "plt.ylim([0.3, 1.02])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
