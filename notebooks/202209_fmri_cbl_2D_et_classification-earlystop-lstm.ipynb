{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import logging\n",
    "from os import path\n",
    "from glob import glob\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from utils import * \n",
    "today=date.today().strftime(\"%Y%m%d\")\n",
    "\n",
    "seed=38\n",
    "\n",
    "# Load matplotlib inline extension\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 158, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob('/data/elekin/data/results/fmri/preproc/cbl/*/cbl_extracted_ts_{}.csv'.format(today))\n",
    "data = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=None)\n",
    "    data.append(df.values)\n",
    "features = np.array(data)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Level3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T023</th>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T018</th>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C084</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C049</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C257</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Level  Level3\n",
       "T023   18.0    13.0\n",
       "T018    5.0    12.0\n",
       "C084    0.0    11.0\n",
       "C049    0.0    11.0\n",
       "C257    0.0    11.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levels = pd.read_csv('/data/elekin/data/results/fmri/preproc/cbl/cbl_levels_{}.csv'.format(today),index_col=0)\n",
    "levels.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C113    0\n",
       "T069    1\n",
       "C132    0\n",
       "C503    0\n",
       "C103    0\n",
       "Name: Level, dtype: int16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = (levels.Level > 0).astype(np.int16)\n",
    "labels.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,labels,test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(name):\n",
    "    return [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=100, min_delta=0.001),\n",
    "      ]\n",
    "\n",
    "def compile_and_fit(model, name, optimizer=None, max_epochs=1000):\n",
    "    if optimizer is None:\n",
    "        optimizer = get_optimizer()\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    history = model.fit(X_train, y_train, batch_size=None, use_multiprocessing=True, \n",
    "                        validation_data=(X_test,y_test), epochs=max_epochs, callbacks=get_callbacks(name),\n",
    "                        verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many models train better if you gradually reduce the learning rate during training. \n",
    "Use optimizers.schedules to reduce the learning rate over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = 1\n",
    "FEATURES = features.shape[1]\n",
    "CHANNELS = features.shape[2]\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  1e-6,\n",
    "  decay_steps=STEPS_PER_EPOCH*1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "\n",
    "def get_optimizer():\n",
    "    return tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24)                5088      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                400       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,633\n",
      "Trainable params: 5,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10000\n",
      "3/3 [==============================] - 1s 144ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 2/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 3/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 4/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 5/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 6/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 7/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 8/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 9/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 10/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 11/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 12/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 13/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 14/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 15/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 16/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 17/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 18/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 19/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 20/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 21/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 22/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 23/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 24/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 25/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 26/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 27/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 28/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 29/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 30/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 31/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 32/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 33/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 34/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 35/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 36/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 37/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 38/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 39/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 40/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 41/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 42/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 43/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 44/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 45/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 46/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 47/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 48/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 49/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 50/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 51/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 53/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 54/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 55/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 56/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 57/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 58/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 59/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 60/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 61/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 62/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 63/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 64/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 65/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 66/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 67/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 68/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 69/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 70/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 71/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 72/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 73/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 74/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 75/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 76/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 77/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 78/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 79/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 80/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 81/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 82/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 83/10000\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 84/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 85/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 86/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 87/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 88/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 89/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 90/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4429 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 91/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4571 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 92/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4571 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 93/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4571 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 94/10000\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6932 - accuracy: 0.4571 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 95/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6932 - accuracy: 0.4571 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 96/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4571 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 97/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4571 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 98/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4571 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 99/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4571 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 100/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4571 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 101/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.4571 - val_loss: 0.6933 - val_accuracy: 0.4000\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "lstm = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(24, activation=tf.nn.tanh,input_shape=[FEATURES, CHANNELS]),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])\n",
    "size_histories['lstm/tiny'] = compile_and_fit(lstm, \"lstm/tiny\", optimizer=None, max_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 158, 256)          291840    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                4112      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 821,409\n",
      "Trainable params: 821,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10000\n",
      "3/3 [==============================] - 2s 252ms/step - loss: 0.6938 - accuracy: 0.4714 - val_loss: 0.6927 - val_accuracy: 0.6571\n",
      "Epoch 2/10000\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6938 - accuracy: 0.4714 - val_loss: 0.6927 - val_accuracy: 0.6571\n",
      "Epoch 3/10000\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6938 - accuracy: 0.4714 - val_loss: 0.6927 - val_accuracy: 0.6571\n",
      "Epoch 4/10000\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6938 - accuracy: 0.4714 - val_loss: 0.6927 - val_accuracy: 0.6571\n",
      "Epoch 5/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6938 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 6/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6938 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 7/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6938 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 8/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6938 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 9/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6937 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 10/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6937 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 11/10000\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6937 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 12/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6937 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 13/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6937 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 14/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6937 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 15/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6937 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 16/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6937 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 17/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6937 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 18/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6937 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 19/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6937 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 20/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6937 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 21/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6937 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 22/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6937 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 23/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6937 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 24/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 25/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 26/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 27/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 28/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 29/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 30/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 31/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 32/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 33/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 34/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 35/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 36/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 37/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 38/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 39/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 40/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 41/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 42/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 43/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 44/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6936 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 45/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6935 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 46/10000\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6935 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 47/10000\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6935 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 48/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6935 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 49/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6935 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 50/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6935 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6935 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 52/10000\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6935 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 53/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6935 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 54/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6935 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6000\n",
      "Epoch 55/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6935 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6000\n",
      "Epoch 56/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6935 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6000\n",
      "Epoch 57/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6935 - accuracy: 0.4714 - val_loss: 0.6928 - val_accuracy: 0.6000\n",
      "Epoch 58/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6935 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6000\n",
      "Epoch 59/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6935 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6000\n",
      "Epoch 60/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6935 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6000\n",
      "Epoch 61/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6935 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6000\n",
      "Epoch 62/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6935 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6000\n",
      "Epoch 63/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6935 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6000\n",
      "Epoch 64/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6934 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6000\n",
      "Epoch 65/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6934 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6000\n",
      "Epoch 66/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6934 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 67/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6934 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 68/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6934 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 69/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6934 - accuracy: 0.4857 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 70/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 71/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 72/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 73/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6934 - accuracy: 0.5143 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 74/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6934 - accuracy: 0.5143 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 75/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6934 - accuracy: 0.5143 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 76/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6934 - accuracy: 0.5143 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 77/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6934 - accuracy: 0.5143 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 78/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6934 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 79/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6934 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 80/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6934 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 81/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6934 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 82/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6934 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 83/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6934 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 84/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6934 - accuracy: 0.5143 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 85/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6934 - accuracy: 0.5143 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 86/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6934 - accuracy: 0.5143 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 87/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6933 - accuracy: 0.5143 - val_loss: 0.6928 - val_accuracy: 0.6571\n",
      "Epoch 88/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6933 - accuracy: 0.5143 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 89/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6933 - accuracy: 0.5143 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 90/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6933 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 91/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6933 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 92/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6933 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 93/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6933 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6000\n",
      "Epoch 94/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6933 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6000\n",
      "Epoch 95/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6933 - accuracy: 0.5143 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 96/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6933 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 97/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6933 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 98/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6933 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 99/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6933 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 100/10000\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6933 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6286\n",
      "Epoch 101/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6933 - accuracy: 0.5286 - val_loss: 0.6928 - val_accuracy: 0.6000\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(seed) # establecemos la semilla para tensorflow\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "small = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True, activation=tf.nn.tanh,input_shape=[FEATURES, CHANNELS]),\n",
    "    tf.keras.layers.LSTM(256, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])\n",
    "size_histories['lstm/small'] = compile_and_fit(small, \"lstm/small\", optimizer=None, max_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 2.15)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_report(size_histories, 'loss')\n",
    "plt.ylim([0., 2.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3, 1.02)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_report(size_histories)\n",
    "plt.ylim([0.3, 1.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer_histories = {}\n",
    "#regularizer_histories['tiny'] = size_histories['lstm/tiny']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 158, 512)         583680    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 512)              1574912   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                8208      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,166,945\n",
      "Trainable params: 2,166,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10000\n",
      "3/3 [==============================] - 4s 692ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 2/10000\n",
      "3/3 [==============================] - 1s 519ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 3/10000\n",
      "3/3 [==============================] - 1s 509ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 4/10000\n",
      "3/3 [==============================] - 2s 534ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 5/10000\n",
      "3/3 [==============================] - 1s 513ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 6/10000\n",
      "3/3 [==============================] - 2s 531ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 7/10000\n",
      "3/3 [==============================] - 2s 551ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 8/10000\n",
      "3/3 [==============================] - 1s 513ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 9/10000\n",
      "3/3 [==============================] - 1s 507ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 10/10000\n",
      "3/3 [==============================] - 1s 523ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 11/10000\n",
      "3/3 [==============================] - 2s 530ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 12/10000\n",
      "3/3 [==============================] - 2s 519ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 13/10000\n",
      "3/3 [==============================] - 1s 519ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 14/10000\n",
      "3/3 [==============================] - 1s 510ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 15/10000\n",
      "3/3 [==============================] - 2s 521ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 16/10000\n",
      "3/3 [==============================] - 2s 524ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 17/10000\n",
      "3/3 [==============================] - 2s 536ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 18/10000\n",
      "3/3 [==============================] - 2s 523ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 19/10000\n",
      "3/3 [==============================] - 1s 519ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 20/10000\n",
      "3/3 [==============================] - 1s 516ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 21/10000\n",
      "3/3 [==============================] - 1s 524ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 22/10000\n",
      "3/3 [==============================] - 1s 508ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 23/10000\n",
      "3/3 [==============================] - 2s 533ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 24/10000\n",
      "3/3 [==============================] - 2s 532ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 25/10000\n",
      "3/3 [==============================] - 1s 508ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 26/10000\n",
      "3/3 [==============================] - 1s 511ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 27/10000\n",
      "3/3 [==============================] - 2s 543ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 28/10000\n",
      "3/3 [==============================] - 1s 520ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 29/10000\n",
      "3/3 [==============================] - 1s 518ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 30/10000\n",
      "3/3 [==============================] - 2s 532ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 31/10000\n",
      "3/3 [==============================] - 2s 533ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 32/10000\n",
      "3/3 [==============================] - 1s 513ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 33/10000\n",
      "3/3 [==============================] - 1s 511ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 34/10000\n",
      "3/3 [==============================] - 1s 499ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 35/10000\n",
      "3/3 [==============================] - 1s 522ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 36/10000\n",
      "3/3 [==============================] - 2s 518ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 37/10000\n",
      "3/3 [==============================] - 2s 539ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 38/10000\n",
      "3/3 [==============================] - 1s 518ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 39/10000\n",
      "3/3 [==============================] - 2s 530ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 40/10000\n",
      "3/3 [==============================] - 2s 532ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 41/10000\n",
      "3/3 [==============================] - 2s 522ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 42/10000\n",
      "3/3 [==============================] - 2s 528ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/10000\n",
      "3/3 [==============================] - 1s 510ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 44/10000\n",
      "3/3 [==============================] - 2s 533ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 45/10000\n",
      "3/3 [==============================] - 2s 539ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 46/10000\n",
      "3/3 [==============================] - 1s 510ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 47/10000\n",
      "3/3 [==============================] - 1s 515ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 48/10000\n",
      "3/3 [==============================] - 1s 514ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 49/10000\n",
      "3/3 [==============================] - 1s 516ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 50/10000\n",
      "3/3 [==============================] - 1s 512ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 51/10000\n",
      "3/3 [==============================] - 1s 512ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 52/10000\n",
      "3/3 [==============================] - 1s 515ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 53/10000\n",
      "3/3 [==============================] - 1s 516ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 54/10000\n",
      "3/3 [==============================] - 2s 518ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 55/10000\n",
      "3/3 [==============================] - 1s 503ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 56/10000\n",
      "3/3 [==============================] - 1s 518ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 57/10000\n",
      "3/3 [==============================] - 1s 530ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4571\n",
      "Epoch 58/10000\n",
      "3/3 [==============================] - 1s 524ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4571\n",
      "Epoch 59/10000\n",
      "3/3 [==============================] - 1s 510ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4571\n",
      "Epoch 60/10000\n",
      "3/3 [==============================] - 2s 537ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4571\n",
      "Epoch 61/10000\n",
      "3/3 [==============================] - 2s 527ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4571\n",
      "Epoch 62/10000\n",
      "3/3 [==============================] - 1s 509ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 63/10000\n",
      "3/3 [==============================] - 2s 516ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 64/10000\n",
      "3/3 [==============================] - 2s 518ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 65/10000\n",
      "3/3 [==============================] - 2s 528ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 66/10000\n",
      "3/3 [==============================] - 2s 526ms/step - loss: 0.6930 - accuracy: 0.4857 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 67/10000\n",
      "3/3 [==============================] - 1s 508ms/step - loss: 0.6930 - accuracy: 0.4857 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 68/10000\n",
      "3/3 [==============================] - 2s 522ms/step - loss: 0.6930 - accuracy: 0.4857 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 69/10000\n",
      "3/3 [==============================] - 1s 517ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 70/10000\n",
      "3/3 [==============================] - 1s 511ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 71/10000\n",
      "3/3 [==============================] - 1s 518ms/step - loss: 0.6930 - accuracy: 0.4857 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 72/10000\n",
      "3/3 [==============================] - 1s 520ms/step - loss: 0.6930 - accuracy: 0.4857 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 73/10000\n",
      "3/3 [==============================] - 1s 513ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 74/10000\n",
      "3/3 [==============================] - 1s 518ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 75/10000\n",
      "3/3 [==============================] - 2s 528ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 76/10000\n",
      "3/3 [==============================] - 1s 522ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 77/10000\n",
      "3/3 [==============================] - 2s 541ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 78/10000\n",
      "3/3 [==============================] - 2s 530ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 79/10000\n",
      "3/3 [==============================] - 2s 529ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 80/10000\n",
      "3/3 [==============================] - 1s 515ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 81/10000\n",
      "3/3 [==============================] - 2s 523ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 82/10000\n",
      "3/3 [==============================] - 1s 512ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 83/10000\n",
      "3/3 [==============================] - 1s 521ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 84/10000\n",
      "3/3 [==============================] - 2s 523ms/step - loss: 0.6930 - accuracy: 0.5143 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 85/10000\n",
      "3/3 [==============================] - 1s 508ms/step - loss: 0.6930 - accuracy: 0.5286 - val_loss: 0.6932 - val_accuracy: 0.4571\n",
      "Epoch 86/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.6929 - accuracy: 0.6562"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\u001b[38;5;66;03m# para evitar que entrenamientos annteriores afecten\u001b[39;00m\n\u001b[1;32m      3\u001b[0m lstm \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      4\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mBidirectional(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLSTM(\u001b[38;5;241m256\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, activation\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu),input_shape\u001b[38;5;241m=\u001b[39m[FEATURES, CHANNELS]),\n\u001b[1;32m      5\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mBidirectional(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLSTM(\u001b[38;5;241m256\u001b[39m, activation\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu)),\n\u001b[1;32m      6\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu),\n\u001b[1;32m      7\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m8\u001b[39m, activation\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu),\n\u001b[1;32m      8\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msigmoid)])\n\u001b[0;32m----> 9\u001b[0m regularizer_histories[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbi-tiny\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_and_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlstm/regularizers/bi-tiny\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mcompile_and_fit\u001b[0;34m(model, name, optimizer, max_epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m---> 11\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(seed) # establecemos la semilla para tensorflow\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "lstm = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True, activation=tf.nn.relu),input_shape=[FEATURES, CHANNELS]),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, activation=tf.nn.relu)),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])\n",
    "regularizer_histories['bi-tiny'] = compile_and_fit(lstm, \"lstm/regularizers/bi-tiny\", optimizer=None, max_epochs=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop out\n",
    "dr=0.2\n",
    "tf.random.set_seed(seed) # establecemos la semilla para tensorflow\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "lstm = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True, activation=tf.nn.tanh,input_shape=[FEATURES, CHANNELS]),\n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.LSTM(128, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)],)\n",
    "regularizer_histories['drop-tiny'] = compile_and_fit(lstm, \"lstm/regularizers/drp-tiny\", optimizer=None, max_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop out\n",
    "dr=0.25\n",
    "lr=1e-6\n",
    "tf.random.set_seed(seed) # establecemos la semilla para tensorflow\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "lstm = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(24, activation=tf.nn.tanh, kernel_regularizer=regularizers.l2(lr), input_shape=[FEATURES, CHANNELS]),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(lr)),\n",
    "    tf.keras.layers.Dense(8, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(lr)),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, kernel_regularizer=regularizers.l2(lr))],)\n",
    "regularizer_histories['kernel'] = compile_and_fit(lstm, \"lstm/regularizers/kernel-reg\", optimizer=None, max_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_report(regularizer_histories,'loss')\n",
    "plt.ylim([0., 2.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_report(regularizer_histories)\n",
    "plt.ylim([0.3, 1.02])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
